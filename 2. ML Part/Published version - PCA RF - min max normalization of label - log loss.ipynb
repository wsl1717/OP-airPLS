{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75363c-a1dd-4a7b-8b60-416f1f65fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy import integrate\n",
    "import warnings\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5407b9-07eb-4873-ac76-3d198f29d201",
   "metadata": {},
   "source": [
    "## airPLS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599acd1a-c45f-4e4d-90dc-8357731bc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhittakerSmooth(x, w, lambda_, differences=1):\n",
    "    \"\"\"Whittaker smoothing function, internal to airPLS algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input signal\n",
    "    w : array-like\n",
    "        Weights array\n",
    "    lambda_ : float\n",
    "        Smoothing parameter\n",
    "    differences : int\n",
    "        Order of differences\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    background : array-like\n",
    "        Smoothed signal\n",
    "    \"\"\"\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    E = eye(m, format='csc')\n",
    "    for i in range(differences):\n",
    "        E = E[1:] - E[:-1]\n",
    "    W = diags(w, 0, shape=(m,m))\n",
    "    \n",
    "    A = csc_matrix(W + (lambda_ * E.T * E))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    \n",
    "    try:\n",
    "        background = spsolve(A, B)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spsolve: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    return np.array(background), W, A, B\n",
    "\n",
    "def airPLS(x, lambda_=100, porder=1, itermax=15, tol=0.001, normalization=False):\n",
    "    \"\"\"Adaptive iteratively reweighted penalized least squares for baseline correction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input spectrum\n",
    "    lambda_ : float\n",
    "        Smoothing parameter, controls the smoothness of the fitted baseline\n",
    "    porder : int\n",
    "        Order of differences\n",
    "    itermax : int\n",
    "        Maximum number of iterations\n",
    "    tol : float\n",
    "        Tolerance parameter for convergence\n",
    "    normalization : bool\n",
    "        Whether to apply normalization to the input spectrum\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    z : array-like\n",
    "        Baseline estimation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        m = x.shape[0]\n",
    "        if normalization:\n",
    "            original_spectrum = x.copy()\n",
    "            # Try auto area normalization\n",
    "            wavenumber = np.arange(m)  # Fake wavenumber for normalization\n",
    "            # Check for NaN or inf values\n",
    "            if np.any(np.isnan(x)) or np.any(np.isnan(wavenumber)) or np.any(np.isinf(x)) or np.any(np.isinf(wavenumber)):\n",
    "                print(\"Warning: NaN or inf values detected in the data\")\n",
    "            else:\n",
    "                # Calculate the area using trapezoidal rule\n",
    "                scaling_factor = integrate.trapz(x, wavenumber)\n",
    "                if scaling_factor == 0:\n",
    "                    scaling_factor = 1\n",
    "                    print(\"Warning: Area is zero, using original spectrum\")\n",
    "                \n",
    "                # Normalize the spectrum\n",
    "                normalized_x = x / scaling_factor\n",
    "        else:\n",
    "            normalized_x = x  # Just to avoid undefined variable errors\n",
    "        \n",
    "        weights = np.ones(m)\n",
    "\n",
    "        for i in range(1, itermax+1):\n",
    "            if normalization:\n",
    "                z, W, A, B = WhittakerSmooth(normalized_x, weights, lambda_, porder)\n",
    "            else:\n",
    "                z, W, A, B = WhittakerSmooth(x, weights, lambda_, porder)\n",
    "                \n",
    "            if z is None:\n",
    "                print(f\"WhittakerSmooth failed at iteration {i}\")\n",
    "                return None\n",
    "            \n",
    "            if normalization:\n",
    "                d = normalized_x - z\n",
    "            else:\n",
    "                d = x - z\n",
    "            dssn = np.abs(d[d<0].sum())\n",
    "\n",
    "            if normalization:\n",
    "                if(dssn < tol * (abs(normalized_x)).sum() or i == itermax):\n",
    "                    break\n",
    "            else:\n",
    "                if(dssn < tol * (abs(x)).sum() or i == itermax):\n",
    "                    break\n",
    "                    \n",
    "            weights[d>=0] = 0\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings(record=True) as caught_warnings:\n",
    "                    warnings.simplefilter(\"always\")\n",
    "                    if np.isinf(dssn):\n",
    "                        dssn = np.finfo(dssn.dtype).max\n",
    "                    exp_term = i * np.abs(d[d<0]) / dssn\n",
    "                    max_exp = 709  # round(np.log(np.finfo(float).max))\n",
    "                    exp_term = np.minimum(exp_term, max_exp)\n",
    "                    weights[d<0] = np.exp(exp_term)\n",
    "                    if caught_warnings:\n",
    "                        warning = caught_warnings[0]\n",
    "                        print(f\"Warning at iteration {i} in exp calculation: {warning.message}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception at iteration {i} in exp calculation: {e}\")\n",
    "                if normalization:\n",
    "                    normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "                    normalized_predicted_x = normalized_original_spectrum - z\n",
    "                    unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "                    z = original_spectrum - unnormalized_predicted_x\n",
    "                return z\n",
    "\n",
    "            try:\n",
    "                if np.isinf(dssn):\n",
    "                    dssn = np.finfo(dssn.dtype).max\n",
    "                weights[0] = np.exp(i * (d[d<0]).max() / dssn)\n",
    "            except Warning as warn:\n",
    "                print(f\"Warning at iteration {i} in w[0] calculation: {warn}\")\n",
    "                if normalization:\n",
    "                    normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "                    normalized_predicted_x = normalized_original_spectrum - z\n",
    "                    unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "                    z = original_spectrum - unnormalized_predicted_x\n",
    "                return z\n",
    "\n",
    "            weights[-1] = weights[0]\n",
    "\n",
    "        if normalization:\n",
    "            normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "            normalized_predicted_x = normalized_original_spectrum - z\n",
    "            unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "            z = original_spectrum - unnormalized_predicted_x\n",
    "        return z\n",
    "    except Exception as e:\n",
    "        print(f\"Error in airPLS: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f53bc-8f47-40f0-b47e-f684939945ec",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145343c8-2227-4a4a-a0ba-82240eb6fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexible_load_model_components(model_dir, model_file=None, pca_file=None, \n",
    "                                  lambda_scaler_file=None, tau_scaler_file=None):\n",
    "    \"\"\"Load model components with flexible file naming.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_dir : str\n",
    "        Base directory for model files\n",
    "    model_file : str or None\n",
    "        Name of model file (or full path if not in model_dir)\n",
    "    pca_file : str or None\n",
    "        Name of PCA file (or full path if not in model_dir)\n",
    "    lambda_scaler_file : str or None\n",
    "        Name of lambda scaler file (or full path if not in model_dir)\n",
    "    tau_scaler_file : str or None\n",
    "        Name of tau scaler file (or full path if not in model_dir)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model, pca, lambda_scaler, tau_scaler : Model components or None if not found\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    model = None\n",
    "    pca = None\n",
    "    lambda_scaler = None\n",
    "    tau_scaler = None\n",
    "    \n",
    "    # Define default filenames\n",
    "    default_model_file = \"PCA_RF_model.pkl\"\n",
    "    default_pca_file = \"pca_transform.pkl\"\n",
    "    default_lambda_scaler_file = \"lambda_scaler.pkl\"\n",
    "    default_tau_scaler_file = \"tau_scaler.pkl\"\n",
    "    \n",
    "    # Use default filenames if not specified\n",
    "    if not model_file:\n",
    "        model_file = default_model_file\n",
    "    if not pca_file:\n",
    "        pca_file = default_pca_file\n",
    "    if not lambda_scaler_file:\n",
    "        lambda_scaler_file = default_lambda_scaler_file\n",
    "    if not tau_scaler_file:\n",
    "        tau_scaler_file = default_tau_scaler_file\n",
    "    \n",
    "    # Determine if paths are absolute or relative to model_dir\n",
    "    model_path = model_file if os.path.isabs(model_file) else os.path.join(model_dir, model_file)\n",
    "    pca_path = pca_file if os.path.isabs(pca_file) else os.path.join(model_dir, pca_file)\n",
    "    lambda_scaler_path = lambda_scaler_file if os.path.isabs(lambda_scaler_file) else os.path.join(model_dir, lambda_scaler_file)\n",
    "    tau_scaler_path = tau_scaler_file if os.path.isabs(tau_scaler_file) else os.path.join(model_dir, tau_scaler_file)\n",
    "    \n",
    "    # Try to load each component and report success/failure\n",
    "    print(f\"Looking for model files...\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Loading model from: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            print(f\"✓ Model loaded successfully: {type(model).__name__}\")\n",
    "        else:\n",
    "            print(f\"✗ Model file not found: {model_path}\")\n",
    "            # Try to find any .pkl files in the directory\n",
    "            pkl_files = [f for f in os.listdir(model_dir) if f.endswith('.pkl')]\n",
    "            if pkl_files:\n",
    "                print(f\"  Found these .pkl files in {model_dir}:\")\n",
    "                for f in pkl_files:\n",
    "                    print(f\"  - {f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading model: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(pca_path):\n",
    "            print(f\"Loading PCA from: {pca_path}\")\n",
    "            pca = joblib.load(pca_path)\n",
    "            print(f\"✓ PCA loaded successfully\")\n",
    "            if hasattr(pca, 'n_components_'):\n",
    "                print(f\"  PCA components: {pca.n_components_}\")\n",
    "            if hasattr(pca, 'mean_'):\n",
    "                print(f\"  PCA expects data of length: {len(pca.mean_)}\")\n",
    "        else:\n",
    "            print(f\"✗ PCA file not found: {pca_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading PCA: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(lambda_scaler_path):\n",
    "            print(f\"Loading lambda scaler from: {lambda_scaler_path}\")\n",
    "            lambda_scaler = joblib.load(lambda_scaler_path)\n",
    "            print(f\"✓ Lambda scaler loaded successfully\")\n",
    "        else:\n",
    "            print(f\"✗ Lambda scaler file not found: {lambda_scaler_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading lambda scaler: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(tau_scaler_path):\n",
    "            print(f\"Loading tau scaler from: {tau_scaler_path}\")\n",
    "            tau_scaler = joblib.load(tau_scaler_path)\n",
    "            print(f\"✓ Tau scaler loaded successfully\")\n",
    "        else:\n",
    "            print(f\"✗ Tau scaler file not found: {tau_scaler_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading tau scaler: {str(e)}\")\n",
    "    \n",
    "    # Final check\n",
    "    if model is None or pca is None or lambda_scaler is None or tau_scaler is None:\n",
    "        print(\"\\n⚠️ Not all model components were loaded successfully.\")\n",
    "        return None, None, None, None\n",
    "    else:\n",
    "        print(\"\\n✓ All model components loaded successfully!\")\n",
    "        return model, pca, lambda_scaler, tau_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d724b1-a82d-4639-83bf-60c6dc3dad1e",
   "metadata": {},
   "source": [
    "## Spectrum Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909ef88-3f8a-4ae9-9c3c-e5d1b9b80f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrum(file_path, spectrum_idx=1):\n",
    "    \"\"\"Load a spectrum from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing spectra\n",
    "    spectrum_idx : int\n",
    "        Index of the spectrum to load (1-based index for columns)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    wavenumbers : array-like\n",
    "        Wavenumber values\n",
    "    spectrum : array-like\n",
    "        Intensity values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading spectrum from {file_path}\")\n",
    "        # Load CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Print column names for debugging\n",
    "        print(f\"CSV columns: {data.columns.tolist()}\")\n",
    "        \n",
    "        # Assuming first column is wavenumbers and subsequent columns are spectra\n",
    "        wavenumbers = data.iloc[:, 0].values\n",
    "        \n",
    "        # Check if spectrum_idx is valid\n",
    "        if spectrum_idx >= data.shape[1]:\n",
    "            print(f\"Warning: spectrum_idx {spectrum_idx} is out of range. Using column 1 instead.\")\n",
    "            spectrum_idx = 1\n",
    "            \n",
    "        spectrum = data.iloc[:, spectrum_idx].values\n",
    "        \n",
    "        # Print some basic info\n",
    "        print(f\"Loaded spectrum with {len(spectrum)} data points\")\n",
    "        print(f\"Wavenumber range: {wavenumbers[0]} to {wavenumbers[-1]}\")\n",
    "        \n",
    "        # Check for NaN or Inf values\n",
    "        if np.any(np.isnan(spectrum)) or np.any(np.isinf(spectrum)):\n",
    "            print(\"Warning: NaN or Inf values found in spectrum. Replacing with zeros.\")\n",
    "            spectrum = np.nan_to_num(spectrum, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return wavenumbers, spectrum\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading spectrum: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114e261-22c9-4b42-8bb9-bb0572e6afea",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7caaac-186a-402d-92d1-40e6d73621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_airpls_params(spectrum, pca, model, lambda_scaler, tau_scaler):\n",
    "    \"\"\"Predict optimal airPLS parameters for a given spectrum using the pre-trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spectrum : array-like\n",
    "        Input spectrum\n",
    "    pca : PCA object\n",
    "        Pre-trained PCA transformation\n",
    "    model : sklearn model\n",
    "        Pre-trained prediction model\n",
    "    lambda_scaler : MinMaxScaler\n",
    "        Scaler for lambda parameter\n",
    "    tau_scaler : MinMaxScaler\n",
    "        Scaler for tau parameter\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lambda_val : float\n",
    "        Predicted lambda parameter\n",
    "    tau_val : float\n",
    "        Predicted tau parameter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make a copy of the spectrum to avoid modifying the original\n",
    "        spectrum_copy = spectrum.copy()\n",
    "        \n",
    "        # Check if PCA object has the expected length attribute\n",
    "        if hasattr(pca, 'mean_'):\n",
    "            expected_length = len(pca.mean_)\n",
    "            print(f\"Expected spectrum length from PCA: {expected_length}\")\n",
    "            print(f\"Actual spectrum length: {len(spectrum_copy)}\")\n",
    "            \n",
    "            # Handle spectrum length mismatch if needed\n",
    "            if len(spectrum_copy) != expected_length:\n",
    "                if len(spectrum_copy) > expected_length:\n",
    "                    print(f\"Truncating spectrum from {len(spectrum_copy)} to {expected_length} points\")\n",
    "                    spectrum_copy = spectrum_copy[:expected_length]\n",
    "                else:\n",
    "                    print(f\"Padding spectrum from {len(spectrum_copy)} to {expected_length} points\")\n",
    "                    spectrum_copy = np.pad(spectrum_copy, \n",
    "                                         (0, expected_length - len(spectrum_copy)), \n",
    "                                         mode='constant',\n",
    "                                         constant_values=0)\n",
    "        else:\n",
    "            print(\"Warning: PCA object doesn't have mean_ attribute. Proceeding with original spectrum length.\")\n",
    "        \n",
    "        # Reshape for model input (flatten in case it's not already 1D)\n",
    "        X = spectrum_copy.reshape(1, -1)\n",
    "        \n",
    "        print(f\"Input shape for PCA: {X.shape}\")\n",
    "        \n",
    "        # Apply PCA transformation\n",
    "        X_pca = pca.transform(X)\n",
    "        \n",
    "        print(f\"PCA transformed shape: {X_pca.shape}\")\n",
    "        \n",
    "        # Make prediction (normalized values)\n",
    "        predicted_normalized = model.predict(X_pca)\n",
    "        predicted_normalized = np.clip(predicted_normalized, 0, 1)  # Clip to [0, 1]\n",
    "        \n",
    "        print(f\"Predicted normalized values: {predicted_normalized}\")\n",
    "        \n",
    "        # Inverse transform to get original scale (log values)\n",
    "        lambda_log = lambda_scaler.inverse_transform(predicted_normalized[:, 0].reshape(-1, 1))\n",
    "        tau_log = tau_scaler.inverse_transform(predicted_normalized[:, 1].reshape(-1, 1))\n",
    "        \n",
    "        # Convert from log to linear\n",
    "        lambda_val = 10**lambda_log[0, 0]\n",
    "        tau_val = 10**tau_log[0, 0]\n",
    "        \n",
    "        return lambda_val, tau_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "def apply_baseline_correction(wavenumbers, spectrum, lambda_val, tau_val, porder=2, itermax=1000): #p=2为默认的参数，调用的时候赋值为多少就是多少\n",
    "    \"\"\"Apply baseline correction using the airPLS algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wavenumbers : array-like\n",
    "        Wavenumber values\n",
    "    spectrum : array-like\n",
    "        Intensity values\n",
    "    lambda_val : float\n",
    "        Lambda parameter for airPLS\n",
    "    tau_val : float\n",
    "        Tau parameter for airPLS\n",
    "    porder : int\n",
    "        Order of differences\n",
    "    itermax : int\n",
    "        Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    corrected_spectrum : array-like\n",
    "        Baseline-corrected spectrum\n",
    "    baseline : array-like\n",
    "        Estimated baseline\n",
    "    \"\"\"\n",
    "    # Apply airPLS\n",
    "    baseline = airPLS(\n",
    "        spectrum,\n",
    "        lambda_=lambda_val,\n",
    "        porder=porder,\n",
    "        itermax=itermax,\n",
    "        tol=tau_val,\n",
    "        normalization=False\n",
    "    )\n",
    "    \n",
    "    if baseline is None:\n",
    "        print(\"Error: Baseline estimation failed.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate corrected spectrum\n",
    "    corrected_spectrum = spectrum - baseline\n",
    "    \n",
    "    return corrected_spectrum, baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5626dc-2490-487e-8cef-54036e0c6d2c",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17443df1-50e1-4cb5-a8bb-6b9c396d97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum(wavenumbers, spectrum, title=\"Spectrum\", xlabel=\"Wavenumber\", ylabel=\"Intensity\", figsize=(10, 6)):\n",
    "    \"\"\"Plot a spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wavenumbers : array-like\n",
    "        Wavenumber values\n",
    "    spectrum : array-like\n",
    "        Intensity values\n",
    "    title : str\n",
    "        Plot title\n",
    "    xlabel : str\n",
    "        X-axis label\n",
    "    ylabel : str\n",
    "        Y-axis label\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(wavenumbers, spectrum)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_baseline_comparison(wavenumbers, spectrum, baseline_pred, baseline_default=None, \n",
    "                           title=\"Baseline Comparison\", figsize=(12, 10)):\n",
    "    \"\"\"Plot baseline comparison and correction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wavenumbers : array-like\n",
    "        Wavenumber values\n",
    "    spectrum : array-like\n",
    "        Original spectrum\n",
    "    baseline_pred : array-like\n",
    "        Baseline predicted using the model\n",
    "    baseline_default : array-like or None\n",
    "        Baseline calculated using default parameters\n",
    "    title : str\n",
    "        Plot title\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Plot original spectrum with baselines\n",
    "    ax1.plot(wavenumbers, spectrum, 'k-', label='Original')\n",
    "    ax1.plot(wavenumbers, baseline_pred, 'r--', label='Predicted Baseline')\n",
    "    \n",
    "    if baseline_default is not None:\n",
    "        ax1.plot(wavenumbers, baseline_default, 'b--', label='Default Baseline')\n",
    "    \n",
    "    ax1.set_title('Spectrum with Fitted Baselines')\n",
    "    ax1.set_xlabel('Wavenumber')\n",
    "    ax1.set_ylabel('Intensity')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot baseline-corrected spectra\n",
    "    corrected_pred = spectrum - baseline_pred\n",
    "    ax2.plot(wavenumbers, corrected_pred, 'r-', label='Predicted Parameters')\n",
    "    \n",
    "    if baseline_default is not None:\n",
    "        corrected_default = spectrum - baseline_default\n",
    "        ax2.plot(wavenumbers, corrected_default, 'b-', label='Default Parameters')\n",
    "    \n",
    "    ax2.set_title('Baseline-Corrected Spectra')\n",
    "    ax2.set_xlabel('Wavenumber')\n",
    "    ax2.set_ylabel('Intensity')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef18e2-24a4-4a44-b793-3ad4cac3f00a",
   "metadata": {},
   "source": [
    "# Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb5cf2-1f6b-47e3-b729-402c71aa3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spectrum(wavenumbers, spectrum, model, pca, lambda_scaler, tau_scaler, \n",
    "                   compare_default=True, save_results=True, output_dir=\"results\"):\n",
    "    \"\"\"Process a spectrum using the model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wavenumbers : array-like\n",
    "        Wavenumber values\n",
    "    spectrum : array-like\n",
    "        Intensity values\n",
    "    model : sklearn model\n",
    "        Pre-trained prediction model\n",
    "    pca : PCA object\n",
    "        Pre-trained PCA transformation\n",
    "    lambda_scaler : MinMaxScaler\n",
    "        Scaler for lambda parameter\n",
    "    tau_scaler : MinMaxScaler\n",
    "        Scaler for tau parameter\n",
    "    compare_default : bool\n",
    "        Whether to compare with default parameters\n",
    "    save_results : bool\n",
    "        Whether to save the results\n",
    "    output_dir : str\n",
    "        Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict : dict\n",
    "        Dictionary containing processing results\n",
    "    \"\"\"\n",
    "    results_dict = {\n",
    "        'wavenumbers': wavenumbers,\n",
    "        'original_spectrum': spectrum,\n",
    "        'lambda_val': None,\n",
    "        'tau_val': None,\n",
    "        'baseline_pred': None,\n",
    "        'corrected_pred': None,\n",
    "        'baseline_default': None,\n",
    "        'corrected_default': None\n",
    "    }\n",
    "    \n",
    "    # Predict optimal parameters\n",
    "    lambda_val, tau_val = predict_airpls_params(\n",
    "        spectrum, pca, model, lambda_scaler, tau_scaler\n",
    "    )\n",
    "    \n",
    "    if lambda_val is None or tau_val is None:\n",
    "        print(\"Error: Failed to predict parameters.\")\n",
    "        return results_dict\n",
    "    \n",
    "    results_dict['lambda_val'] = lambda_val\n",
    "    results_dict['tau_val'] = tau_val\n",
    "    \n",
    "    print(f\"Predicted parameters:\")\n",
    "    print(f\"  Lambda: {lambda_val:.6e}\")\n",
    "    print(f\"  Tau: {tau_val:.6e}\")\n",
    "    \n",
    "    # Apply baseline correction with predicted parameters\n",
    "    corrected_pred, baseline_pred = apply_baseline_correction(\n",
    "        wavenumbers, spectrum, lambda_val, tau_val\n",
    "    )\n",
    "    \n",
    "    if corrected_pred is None or baseline_pred is None:\n",
    "        print(\"Error: Failed to apply baseline correction with predicted parameters.\")\n",
    "        return results_dict\n",
    "    \n",
    "    results_dict['baseline_pred'] = baseline_pred\n",
    "    results_dict['corrected_pred'] = corrected_pred\n",
    "    \n",
    "    # Apply baseline correction with default parameters for comparison\n",
    "    if compare_default:\n",
    "        corrected_default, baseline_default = apply_baseline_correction(\n",
    "            wavenumbers, spectrum, 100, 0.001, porder=1, itermax=15\n",
    "        )\n",
    "        \n",
    "        if corrected_default is not None and baseline_default is not None:\n",
    "            results_dict['baseline_default'] = baseline_default\n",
    "            results_dict['corrected_default'] = corrected_default\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Create DataFrame with results\n",
    "        results_df = pd.DataFrame({\n",
    "            \"Wavenumber\": wavenumbers,\n",
    "            \"Original\": spectrum,\n",
    "            \"Predicted_Baseline\": baseline_pred,\n",
    "            \"Corrected_Predicted\": corrected_pred\n",
    "        })\n",
    "        \n",
    "        if compare_default and baseline_default is not None:\n",
    "            results_df[\"Default_Baseline\"] = baseline_default\n",
    "            results_df[\"Corrected_Default\"] = corrected_default\n",
    "        \n",
    "        # Save to CSV\n",
    "        results_path = os.path.join(output_dir, \"baseline_correction_results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Results saved to {results_path}\")\n",
    "        \n",
    "        # Save parameters\n",
    "        params_df = pd.DataFrame({\n",
    "            \"Parameter\": [\"lambda\", \"tau\"],\n",
    "            \"Value\": [lambda_val, tau_val]\n",
    "        })\n",
    "        \n",
    "        params_path = os.path.join(output_dir, \"predicted_parameters.csv\")\n",
    "        params_df.to_csv(params_path, index=False)\n",
    "        print(f\"Parameters saved to {params_path}\")\n",
    "        \n",
    "    return results_dict\n",
    "\n",
    "def process_multiple_spectra(file_path, model, pca, lambda_scaler, tau_scaler, output_dir=\"results\"):\n",
    "    \"\"\"Process multiple spectra from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing spectra\n",
    "    model : sklearn model\n",
    "        Pre-trained prediction model\n",
    "    pca : PCA object\n",
    "        Pre-trained PCA transformation\n",
    "    lambda_scaler : MinMaxScaler\n",
    "        Scaler for lambda parameter\n",
    "    tau_scaler : MinMaxScaler\n",
    "        Scaler for tau parameter\n",
    "    output_dir : str\n",
    "        Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    all_results : dict\n",
    "        Dictionary containing all processing results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get wavenumbers from first column\n",
    "        wavenumbers = data.iloc[:, 0].values\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Process each spectrum\n",
    "        all_results = {}\n",
    "        all_params = []\n",
    "        \n",
    "        for i in range(1, data.shape[1]):\n",
    "            try:\n",
    "                # Extract spectrum\n",
    "                spectrum = data.iloc[:, i].values\n",
    "                spectrum_name = data.columns[i] if i < len(data.columns) else f\"Spectrum_{i}\"\n",
    "                \n",
    "                print(f\"\\nProcessing {spectrum_name}...\")\n",
    "                \n",
    "                # Create subdirectory for this spectrum\n",
    "                spectrum_dir = os.path.join(output_dir, f\"{spectrum_name}\")\n",
    "                if not os.path.exists(spectrum_dir):\n",
    "                    os.makedirs(spectrum_dir)\n",
    "                \n",
    "                # Process the spectrum\n",
    "                results = process_spectrum(\n",
    "                    wavenumbers, spectrum, model, pca, lambda_scaler, tau_scaler,\n",
    "                    compare_default=True, save_results=False\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                all_results[spectrum_name] = results\n",
    "                \n",
    "                # Add to parameters table\n",
    "                if results['lambda_val'] is not None and results['tau_val'] is not None:\n",
    "                    all_params.append({\n",
    "                        \"Spectrum\": spectrum_name,\n",
    "                        \"Lambda\": results['lambda_val'],\n",
    "                        \"Tau\": results['tau_val']\n",
    "                    })\n",
    "                    \n",
    "                    # Create results DataFrame for this spectrum\n",
    "                    spectrum_df = pd.DataFrame({\n",
    "                        \"Wavenumber\": wavenumbers,\n",
    "                        \"Original\": spectrum,\n",
    "                        \"Baseline\": results['baseline_pred'],\n",
    "                        \"Corrected\": results['corrected_pred']\n",
    "                    })\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    spectrum_path = os.path.join(spectrum_dir, f\"{spectrum_name}_results.csv\")\n",
    "                    spectrum_df.to_csv(spectrum_path, index=False)\n",
    "                    \n",
    "                    # Create and save plot\n",
    "                    fig = plot_baseline_comparison(\n",
    "                        wavenumbers, spectrum, results['baseline_pred'],\n",
    "                        results['baseline_default'] if 'baseline_default' in results else None,\n",
    "                        title=f\"Baseline Correction - {spectrum_name}\",\n",
    "                        figsize=(10, 8)\n",
    "                    )\n",
    "                    \n",
    "                    plot_path = os.path.join(spectrum_dir, f\"{spectrum_name}_plot.png\")\n",
    "                    fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {spectrum_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save all parameters to CSV\n",
    "        if all_params:\n",
    "            params_df = pd.DataFrame(all_params)\n",
    "            params_path = os.path.join(output_dir, \"all_predicted_parameters.csv\")\n",
    "            params_df.to_csv(params_path, index=False)\n",
    "            print(f\"\\nAll parameters saved to {params_path}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e56bc-5e69-43c5-8796-8aad9d2bb504",
   "metadata": {},
   "source": [
    "## Example usage of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0381c-379b-4691-854a-555ff1912a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing model files\n",
    "model_dir = r\"e:\\Desktop\\OP-airPLS\\2. ML Part\"\n",
    "\n",
    "# Individual model component files - you can change these to match your actual filenames\n",
    "# If you leave them as empty strings, the code will look for default filenames\n",
    "model_file = \"PCA RF_model_02132025.pkl\"  # Default: \"PCA_RF_model.pkl\"\n",
    "pca_file = \"pca_transform_model_02132025.pkl\"    # Default: \"pca_transform.pkl\"\n",
    "lambda_scaler_file = \"lambda_scaler_01292025.pkl\"  # Default: \"lambda_scaler.pkl\"\n",
    "tau_scaler_file = \"tau_scaler_01292025.pkl\"     # Default: \"tau_scaler.pkl\"\n",
    "\n",
    "# Sample data and output settings\n",
    "sample_data_path = r\"e:\\Desktop\\OP-airPLS\\2. ML Part\\data.csv\"\n",
    "output_dir = model_dir + \"/results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Try to load model components with the flexible function\n",
    "model, pca, lambda_scaler, tau_scaler = flexible_load_model_components(\n",
    "    model_dir=model_dir,\n",
    "    model_file=model_file if model_file else None,\n",
    "    pca_file=pca_file if pca_file else None,\n",
    "    lambda_scaler_file=lambda_scaler_file if lambda_scaler_file else None,\n",
    "    tau_scaler_file=tau_scaler_file if tau_scaler_file else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd61a09-5f27-4300-bf82-6aa4452aa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process spectrum with loaded model components\n",
    "if model is None or pca is None or lambda_scaler is None or tau_scaler is None:\n",
    "    print(\"Failed to load model components. Execution stopped.\")\n",
    "else:\n",
    "    # Check if sample data exists\n",
    "    if not os.path.exists(sample_data_path):\n",
    "        print(f\"Sample data not found at {sample_data_path}. Execution stopped.\")\n",
    "    else:\n",
    "        # First process a single spectrum as an example\n",
    "        print(\"\\n=== Processing First Spectrum as Example ===\")\n",
    "        wavenumbers, spectrum = load_spectrum(sample_data_path, spectrum_idx=1)\n",
    "        if wavenumbers is None or spectrum is None:\n",
    "            print(\"Failed to load spectrum. Execution stopped.\")\n",
    "        else:\n",
    "            # Plot the original spectrum\n",
    "            plot_spectrum(wavenumbers, spectrum, title=\"Original Sample Spectrum\")\n",
    "            \n",
    "            # Process the spectrum\n",
    "            results = process_spectrum(\n",
    "                wavenumbers, spectrum, model, pca, lambda_scaler, tau_scaler,\n",
    "                compare_default=True, save_results=True, output_dir=output_dir\n",
    "            )\n",
    "            \n",
    "            # Plot baseline comparison\n",
    "            if results['baseline_pred'] is not None:\n",
    "                plot_baseline_comparison(\n",
    "                    wavenumbers, spectrum, results['baseline_pred'], results['baseline_default'],\n",
    "                    title=\"Baseline Comparison and Correction\"\n",
    "                )\n",
    "                \n",
    "            # Automatically process all spectra in the file\n",
    "            print(\"\\n=== Processing All Spectra in the File ===\")\n",
    "            try:\n",
    "                all_results = process_multiple_spectra(\n",
    "                    sample_data_path, model, pca, lambda_scaler, tau_scaler, output_dir=output_dir\n",
    "                )\n",
    "                print(f\"Successfully processed {len(all_results)} spectra in total.\")\n",
    "                print(f\"Results saved to: {output_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing multiple spectra: {str(e)}\")\n",
    "                print(\"You can still examine the individual spectrum results above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791208c-cabb-4578-848a-3b056140c411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
